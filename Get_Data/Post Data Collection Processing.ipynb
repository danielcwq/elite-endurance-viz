{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b0daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielching/miniforge3/envs/selenium_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a95981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielching/miniforge3/lib/python3.10/site-packages/conda_package_streaming/package_streaming.py:25: UserWarning: zstandard could not be imported. Running without .conda support.\n",
      "  warnings.warn(\"zstandard could not be imported. Running without .conda support.\")\n",
      "/Users/danielching/miniforge3/lib/python3.10/site-packages/conda_package_handling/api.py:29: UserWarning: Install zstandard Python bindings for .conda support\n",
      "  _warnings.warn(\"Install zstandard Python bindings for .conda support\")\n",
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1abb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44938b91",
   "metadata": {},
   "source": [
    "# Indiv Activities Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5cf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def detect_wrongly_tagged_paces(input_folder):\n",
    "    \"\"\"\n",
    "    Detect and flag `_indiv_activities` CSV files with unrealistic paces based on headers.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing `_indiv_activities` files.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    flagged_files = []\n",
    "\n",
    "    # Loop through all `_indiv_activities` files\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if '_indiv_activities' in filename and filename.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            try:\n",
    "                # Load the file\n",
    "                data = pd.read_csv(file_path)\n",
    "\n",
    "                # Check if the header contains \"min/mi\"\n",
    "                if 'Pace (min/mi)' in data.columns:\n",
    "                    # Group by Athlete ID to calculate average pace for each athlete\n",
    "                    athlete_groups = data.groupby(['Athlete ID', 'Athlete Name'])\n",
    "\n",
    "                    for (athlete_id, athlete_name), group in athlete_groups:\n",
    "                        avg_pace = group['Pace (min/mi)'].mean()\n",
    "\n",
    "                        # Flag if the average pace is in the 4.xx range\n",
    "                        if 4.00 <= avg_pace < 5.00:\n",
    "                            flagged_files.append({\n",
    "                                'File': filename,\n",
    "                                'Athlete ID': athlete_id,\n",
    "                                'Athlete Name': athlete_name,\n",
    "                                'Average Pace (min/mi)': avg_pace\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Print flagged files and athletes\n",
    "    if flagged_files:\n",
    "        print(\"Potentially wrongly tagged files and athletes:\")\n",
    "        for entry in flagged_files:\n",
    "            print(f\"File: {entry['File']}, Athlete: {entry['Athlete Name']} (ID: {entry['Athlete ID']}), \"\n",
    "                  f\"Avg Pace: {entry['Average Pace (min/mi)']:.2f}\")\n",
    "    else:\n",
    "        print(\"No wrongly tagged files or athletes detected.\")\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2dc8a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potentially wrongly tagged files and athletes:\n",
      "File: batch_45_indiv_activities.csv, Athlete: Sondre Nordstad Moen (ID: 14983457), Avg Pace: 4.98\n",
      "File: batch_39_indiv_activities.csv, Athlete: Niels Laros (ID: 29900318), Avg Pace: 4.53\n",
      "File: batch_46_indiv_activities.csv, Athlete: Kirami Collin Yego (ID: 73487204), Avg Pace: 4.61\n",
      "File: batch_34_indiv_activities.csv, Athlete: Haftom Welday (ID: 23836673), Avg Pace: 4.75\n"
     ]
    }
   ],
   "source": [
    "detect_wrongly_tagged_paces(input_folder='./data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb50f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_individual_activities_by_metric(input_folder, output_min_mi_file, output_min_km_file):\n",
    "    \"\"\"\n",
    "    Merge `_indiv_activities` CSV files into two separate files based on pace metrics.\n",
    "    \n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing `_indiv_activities` files.\n",
    "        output_min_mi_file (str): Path to save the merged data with `min/mi` metric.\n",
    "        output_min_km_file (str): Path to save the merged data with `min/km` metric.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    merged_min_mi = pd.DataFrame()\n",
    "    merged_min_km = pd.DataFrame()\n",
    "    files_processed = 0\n",
    "    skipped_files = 0\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if '_indiv_activities' in filename and filename.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            try:\n",
    "                # Read the current file\n",
    "                current_data = pd.read_csv(file_path)\n",
    "\n",
    "                # Check which metric the file contains\n",
    "                if 'Pace (min/mi)' in current_data.columns:\n",
    "                    # Merge with `min/mi` DataFrame\n",
    "                    merged_min_mi = pd.concat([merged_min_mi, current_data], ignore_index=True)\n",
    "                    print(f\"Processed file with 'min/mi': {filename}\")\n",
    "                elif 'Pace (min/km)' in current_data.columns:\n",
    "                    # Merge with `min/km` DataFrame\n",
    "                    merged_min_km = pd.concat([merged_min_km, current_data], ignore_index=True)\n",
    "                    print(f\"Processed file with 'min/km': {filename}\")\n",
    "                else:\n",
    "                    # Skip files with inconsistent or missing headers\n",
    "                    print(f\"Skipped file due to missing pace metric: {filename}\")\n",
    "                    skipped_files += 1\n",
    "                    continue\n",
    "\n",
    "                files_processed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "                skipped_files += 1\n",
    "\n",
    "    # Save the merged files\n",
    "    if not merged_min_mi.empty:\n",
    "        merged_min_mi.to_csv(output_min_mi_file, index=False)\n",
    "        print(f\"Merged data with 'min/mi' saved to {output_min_mi_file}\")\n",
    "    else:\n",
    "        print(f\"No files with 'min/mi' metric were processed.\")\n",
    "\n",
    "    if not merged_min_km.empty:\n",
    "        merged_min_km.to_csv(output_min_km_file, index=False)\n",
    "        print(f\"Merged data with 'min/km' saved to {output_min_km_file}\")\n",
    "    else:\n",
    "        print(f\"No files with 'min/km' metric were processed.\")\n",
    "\n",
    "    print(f\"\\nTotal files processed: {files_processed}, Skipped files: {skipped_files}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae94c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file with 'min/mi': batch_18_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_9_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_20_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_25_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_53_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_12_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_17_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_6_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_44_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_41_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_38_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_37_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_32_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_13_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_2_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_52_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_7_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_16_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_21_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_8_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_19_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_24_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_36_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_33_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_45_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_40_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_39_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_43_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_30_indiv_activities_30.csv\n",
      "Processed file with 'min/mi': batch_46_indiv_activities.csv\n",
      "Processed file with 'min/km': batch_49_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_35_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_27_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_22_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_4_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_15_indiv_activities.csv\n",
      "Processed file with 'min/km': batch_54_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_10_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_1_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_28_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_51_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_31_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_48_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_34_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_42_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_47_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_14_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_5_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_29_indiv_activities.csv\n",
      "Processed file with 'min/km': batch_50_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_11_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_26_indiv_activities.csv\n",
      "Processed file with 'min/mi': batch_23_indiv_activities.csv\n",
      "Merged data with 'min/mi' saved to ./data/merged_min_mi.csv\n",
      "Merged data with 'min/km' saved to ./data/merged_min_km.csv\n",
      "\n",
      "Total files processed: 53, Skipped files: 0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "merge_individual_activities_by_metric(\n",
    "    input_folder='./data',\n",
    "    output_min_mi_file='./data/merged_min_mi.csv',\n",
    "    output_min_km_file='./data/merged_min_km.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28b6a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_min_mi_to_min_km(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert `Pace (min/mi)` in the input file to `Pace (min/km)` and save the output.\n",
    "    Handles time formats like `1h 14m`, `50m 1s`, and `0m XXs`.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file with `min/mi` metrics.\n",
    "        output_file (str): Path to save the converted CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(input_file)\n",
    "\n",
    "    # Ensure numeric columns are correct\n",
    "    data['Distance (mi)'] = pd.to_numeric(data['Distance (mi)'], errors='coerce').fillna(0).astype(float)\n",
    "    data['Pace (min/mi)'] = pd.to_numeric(data['Pace (min/mi)'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "    # Convert Pace (min/mi) to Pace (min/km)\n",
    "    data['Pace (min/km)'] = data['Pace (min/mi)'] / 1.60934\n",
    "\n",
    "    # Convert Time to decimal minutes\n",
    "    def time_to_minutes(time_str):\n",
    "        try:\n",
    "            hours, minutes, seconds = 0, 0, 0\n",
    "            \n",
    "            # Parse \"1h 14m\" format\n",
    "            if \"h\" in time_str:\n",
    "                parts = time_str.split(\"h\")\n",
    "                hours = int(parts[0].strip())\n",
    "                time_str = parts[1].strip()\n",
    "            \n",
    "            # Parse \"50m 1s\" or \"0m XXs\"\n",
    "            if \"m\" in time_str:\n",
    "                parts = time_str.split(\"m\")\n",
    "                minutes = int(parts[0].strip())\n",
    "                if \"s\" in parts[1]:\n",
    "                    seconds = int(parts[1].replace(\"s\", \"\").strip())\n",
    "            elif \"s\" in time_str:\n",
    "                # Handle \"0m XXs\" or \"XXs\"\n",
    "                seconds = int(time_str.replace(\"s\", \"\").strip())\n",
    "\n",
    "            # Convert to decimal minutes\n",
    "            return round(hours * 60 + minutes + seconds / 60, 2)\n",
    "        except:\n",
    "            return 0  # Default for invalid entries\n",
    "\n",
    "    data['Time (min)'] = data['Time'].apply(time_to_minutes)\n",
    "\n",
    "    # Remove old Time column (optional)\n",
    "    data.drop(columns=['Time'], inplace=True)\n",
    "\n",
    "    # Rename Distance column to `Distance (km)` for consistency\n",
    "    data['Distance (km)'] = data['Distance (mi)'] * 1.60934\n",
    "    data.drop(columns=['Distance (mi)'], inplace=True)\n",
    "\n",
    "    # Save the converted data\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f\"Converted file saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ba4b507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted file saved to ./data/converted_min_km.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "convert_min_mi_to_min_km(\n",
    "    input_file='./data/merged_min_mi.csv',\n",
    "    output_file='./data/converted_min_km.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aceeecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_in_min_km(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert `Time` in the input file with `min/km` metrics into decimal minutes.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file with `min/km` metrics.\n",
    "        output_file (str): Path to save the converted CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(input_file)\n",
    "\n",
    "    # Ensure numeric columns are correct\n",
    "    data['Distance (km)'] = pd.to_numeric(data['Distance (km)'], errors='coerce').fillna(0).astype(float)\n",
    "    data['Pace (min/km)'] = pd.to_numeric(data['Pace (min/km)'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "    # Convert Time to decimal minutes\n",
    "    def time_to_minutes(time_str):\n",
    "        try:\n",
    "            hours, minutes, seconds = 0, 0, 0\n",
    "            \n",
    "            # Parse \"1h 14m\" format\n",
    "            if \"h\" in time_str:\n",
    "                parts = time_str.split(\"h\")\n",
    "                hours = int(parts[0].strip())\n",
    "                time_str = parts[1].strip()\n",
    "            \n",
    "            # Parse \"50m 1s\" or \"0m XXs\"\n",
    "            if \"m\" in time_str:\n",
    "                parts = time_str.split(\"m\")\n",
    "                minutes = int(parts[0].strip())\n",
    "                if \"s\" in parts[1]:\n",
    "                    seconds = int(parts[1].replace(\"s\", \"\").strip())\n",
    "            elif \"s\" in time_str:\n",
    "                # Handle \"0m XXs\" or \"XXs\"\n",
    "                seconds = int(time_str.replace(\"s\", \"\").strip())\n",
    "\n",
    "            # Convert to decimal minutes\n",
    "            return round(hours * 60 + minutes + seconds / 60, 2)\n",
    "        except:\n",
    "            return 0  # Default for invalid entries\n",
    "\n",
    "    data['Time (min)'] = data['Time'].apply(time_to_minutes)\n",
    "\n",
    "    # Remove old Time column (optional)\n",
    "    data.drop(columns=['Time'], inplace=True)\n",
    "\n",
    "    # Save the converted data\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f\"Converted file saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a01a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted file saved to ./data/converted_min_km_with_time.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "convert_time_in_min_km(\n",
    "    input_file='./data/merged_min_km.csv',\n",
    "    output_file='./data/converted_min_km_with_time.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5448f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to ./data/indiv_activities_full.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_and_add_activity_time(file1, file2, output_file):\n",
    "    \"\"\"\n",
    "    Merge two converted files and add `Activity Time (s)` column.\n",
    "\n",
    "    Args:\n",
    "        file1 (str): Path to the first input CSV file.\n",
    "        file2 (str): Path to the second input CSV file.\n",
    "        output_file (str): Path to save the merged CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load both datasets\n",
    "    data1 = pd.read_csv(file1)\n",
    "    data2 = pd.read_csv(file2)\n",
    "\n",
    "    # Merge datasets\n",
    "    merged_data = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "    # Ensure `Time (min)` is numeric\n",
    "    merged_data['Time (min)'] = pd.to_numeric(merged_data['Time (min)'], errors='coerce').fillna(0)\n",
    "\n",
    "    # Add `Activity Time (s)` as an integer column\n",
    "    merged_data['Activity Time (s)'] = (merged_data['Time (min)'] * 60).astype(int)\n",
    "\n",
    "    # Save the merged dataset\n",
    "    merged_data.to_csv(output_file, index=False)\n",
    "    print(f\"Merged file saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "merge_and_add_activity_time(\n",
    "    file1='./data/converted_min_km.csv',\n",
    "    file2='./data/converted_min_km_with_time.csv',\n",
    "    output_file='./data/indiv_activities_full.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f1f2b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/bxcnwybj72j7sr4fzj25fj3c0000gn/T/ipykernel_36834/4158597234.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  activities['Type'] = activities['Type'].replace('VirtualRide', 'Ride')\n",
      "/var/folders/6t/bxcnwybj72j7sr4fzj25fj3c0000gn/T/ipykernel_36834/4158597234.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  activities['Type'] = activities['Type'].apply(lambda x: x if x in valid_types else 'Other')\n",
      "/var/folders/6t/bxcnwybj72j7sr4fzj25fj3c0000gn/T/ipykernel_36834/4158597234.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  activities['Activity Time (s)'] = activities['Activity Time (s)'].fillna(activities['Elapsed Time'])\n",
      "/var/folders/6t/bxcnwybj72j7sr4fzj25fj3c0000gn/T/ipykernel_36834/4158597234.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  activities['Activity Hours'] = activities['Activity Time (s)'] / 3600.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_athlete_name(name: str) -> str:\n",
    "    return \" \".join(name.strip().title().split())\n",
    "\n",
    "def load_and_clean_data():\n",
    "    activities = pd.read_csv(\"./data/indiv_activities_full.csv\")\n",
    "    # Convert to datetime and ensure no timezone issues\n",
    "    activities['Start Date'] = pd.to_datetime(activities['Start Date'], utc=True)\n",
    "    activities['Start Date'] = activities['Start Date'].dt.tz_convert(None)\n",
    "\n",
    "    male_iaaf = pd.read_csv(\"combined_df_male_distanceCAA30Oct (1).csv\")\n",
    "    female_iaaf = pd.read_csv(\"combined_df_female_distanceCAA30Oct.csv\")\n",
    "\n",
    "    male_iaaf['Gender'] = 'Male'\n",
    "    female_iaaf['Gender'] = 'Female'\n",
    "    iaaf = pd.concat([male_iaaf, female_iaaf], ignore_index=True)\n",
    "\n",
    "    iaaf['Competitor'] = iaaf['Competitor'].apply(clean_athlete_name)\n",
    "    activities['Athlete Name'] = activities['Athlete Name'].apply(clean_athlete_name)\n",
    "\n",
    "    return activities, iaaf\n",
    "\n",
    "\n",
    "def filter_activities_by_date(activities: pd.DataFrame, start_date='2024-01-01', weeks=45):\n",
    "    end_date = pd.to_datetime(start_date) + pd.Timedelta(weeks=weeks)\n",
    "    filtered = activities[(activities['Start Date'] >= start_date) & (activities['Start Date'] < end_date)]\n",
    "    return filtered\n",
    "\n",
    "def prepare_activity_types(activities: pd.DataFrame):\n",
    "    # Normalize activity types\n",
    "    # Combine VirtualRide and Ride into Ride\n",
    "    activities['Type'] = activities['Type'].replace('VirtualRide', 'Ride')\n",
    "    # For future-proofing, if we have Swim and Run, they remain as is.\n",
    "    # Any type not in [Run, Ride, Swim] becomes Other\n",
    "    valid_types = ['Run', 'Ride', 'Swim']\n",
    "    activities['Type'] = activities['Type'].apply(lambda x: x if x in valid_types else 'Other')\n",
    "    return activities\n",
    "\n",
    "def compute_activity_hours(activities: pd.DataFrame):\n",
    "    # If Activity Time (s) not available or NA, use Elapsed Time\n",
    "    # Assume Elapsed Time is in seconds (based on the given snippet).\n",
    "    # If Activity Time (s) is NaN, fallback to Elapsed Time.\n",
    "    activities['Activity Time (s)'] = activities['Activity Time (s)'].fillna(activities['Elapsed Time'])\n",
    "    # Convert to hours\n",
    "    activities['Activity Hours'] = activities['Activity Time (s)'] / 3600.0\n",
    "    return activities\n",
    "\n",
    "def aggregate_training_metrics(activities: pd.DataFrame):\n",
    "    # Calculate total and average (per 45 weeks) metrics\n",
    "    grouped = activities.groupby(['Athlete Name', 'Type'], as_index=False).agg({\n",
    "        'Distance (km)': 'sum',\n",
    "        'Activity Hours': 'sum'\n",
    "    })\n",
    "\n",
    "    pivot_dist = grouped.pivot(index='Athlete Name', columns='Type', values='Distance (km)').fillna(0)\n",
    "    pivot_time = grouped.pivot(index='Athlete Name', columns='Type', values='Activity Hours').fillna(0)\n",
    "\n",
    "    # Run metrics\n",
    "    total_run_distance = pivot_dist['Run'] if 'Run' in pivot_dist.columns else pd.Series(0, index=pivot_dist.index)\n",
    "    total_run_hours = pivot_time['Run'] if 'Run' in pivot_time.columns else pd.Series(0, index=pivot_dist.index)\n",
    "\n",
    "    avg_weekly_run_mileage = total_run_distance / 45.0\n",
    "    avg_weekly_run_hours = total_run_hours / 45.0\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Athlete Name': pivot_dist.index,\n",
    "        'Total_Run_Distance_km': total_run_distance,\n",
    "        'Avg_Weekly_Run_Mileage_km': avg_weekly_run_mileage,\n",
    "        'Total_Run_Hours': total_run_hours,\n",
    "        'Avg_Weekly_Run_Hours': avg_weekly_run_hours\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    # Add columns for other activities\n",
    "    # We assume columns might be 'Ride', 'Swim', 'Other'\n",
    "    for activity_type in ['Ride', 'Swim', 'Other']:\n",
    "        if activity_type in pivot_time.columns:\n",
    "            metrics_df[f'Total_{activity_type}_Hours'] = pivot_time[activity_type].values\n",
    "        else:\n",
    "            metrics_df[f'Total_{activity_type}_Hours'] = 0.0\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def merge_iaaf_data(metrics_df: pd.DataFrame, iaaf: pd.DataFrame):\n",
    "    # Aggregate IAAF data\n",
    "    iaaf_agg = iaaf.groupby('Competitor', as_index=False).agg(\n",
    "    Nat=('Nat', 'first'),\n",
    "    Gender=('Gender', 'first'),\n",
    "    Mark=('Mark', lambda x: '|'.join(x.dropna().unique())),\n",
    "    Discipline=('Discipline', lambda x: '|'.join(x.dropna().unique())),\n",
    "    Number_of_events=('Competitor', 'count')\n",
    "    )\n",
    "\n",
    "\n",
    "    merged = metrics_df.merge(iaaf_agg, left_on='Athlete Name', right_on='Competitor', how='left')\n",
    "\n",
    "    # Fill missing values\n",
    "    merged['Nat'] = merged['Nat'].fillna('')\n",
    "    merged['Gender'] = merged['Gender'].fillna('')\n",
    "    merged['Mark'] = merged['Mark'].fillna('')\n",
    "    merged['Discipline'] = merged['Discipline'].fillna('')\n",
    "    merged['Number_of_events'] = merged['Number_of_events'].fillna(0).astype(int)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def check_for_na_and_alert(df: pd.DataFrame):\n",
    "    # Check if any NA present\n",
    "    if df.isna().any().any():\n",
    "        na_cols = df.columns[df.isna().any()].tolist()\n",
    "        print(f\"Warning: The following columns contain NA values: {na_cols}\")\n",
    "    else:\n",
    "        print(\"No NA values found in the final DataFrame.\")\n",
    "\n",
    "\n",
    "activities, iaaf = load_and_clean_data()\n",
    "filtered_activities = filter_activities_by_date(activities)\n",
    "filtered_activities = prepare_activity_types(filtered_activities)\n",
    "filtered_activities = compute_activity_hours(filtered_activities)\n",
    "metrics_df = aggregate_training_metrics(filtered_activities)\n",
    "final_df = merge_iaaf_data(metrics_df, iaaf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dc2c32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The following columns contain NA values: ['Competitor']\n"
     ]
    }
   ],
   "source": [
    "check_for_na_and_alert(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c01e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"cleaned_athlete_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0eede7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_average_run_pace(final_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure no division by zero\n",
    "    # Run pace in min/km = (Total_Run_Hours * 60) / Total_Run_Distance_km\n",
    "    # If distance is zero, set pace to NaN or a default value\n",
    "    final_df['Avg_Run_Pace_min_per_km'] = (final_df['Total_Run_Hours'] * 60) / final_df['Total_Run_Distance_km']\n",
    "    final_df.loc[final_df['Total_Run_Distance_km'] == 0, 'Avg_Run_Pace_min_per_km'] = np.nan\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "281f222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = add_average_run_pace(final_df)\n",
    "final_df.to_csv(\"cleaned_athlete_metadata.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02acc40",
   "metadata": {},
   "source": [
    "# Metadata Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def merge_meta_files_with_batch_debug(input_folder, output_file):\n",
    "    merged_data = pd.DataFrame()\n",
    "    files_processed = 0\n",
    "    skipped_files = 0\n",
    "    consistent_headers = None\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Skip non-CSV files or files containing \"_indiv_activities\" in the name\n",
    "        if not filename.endswith('.csv') or '_indiv_activities' in filename:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Read the current file\n",
    "            current_data = pd.read_csv(file_path)\n",
    "\n",
    "            # Extract the batch number from the filename using regex\n",
    "            batch_match = re.search(r'batch_(\\d+)', filename)\n",
    "            if batch_match:\n",
    "                batch_number = batch_match.group(1)\n",
    "            else:\n",
    "                batch_number = \"Unknown\"\n",
    "                print(f\"Warning: Could not extract batch number from filename {filename}\")\n",
    "\n",
    "            # Add a \"Batch\" column to track the batch source\n",
    "            current_data['Batch'] = batch_number\n",
    "            print(f\"Added 'Batch' column with value '{batch_number}' to file: {filename}\")\n",
    "\n",
    "            # Check and enforce consistent headers\n",
    "            if consistent_headers is None:\n",
    "                consistent_headers = set(current_data.columns)\n",
    "                merged_data = current_data\n",
    "            elif set(current_data.columns) == consistent_headers:\n",
    "                merged_data = pd.concat([merged_data, current_data], ignore_index=True)\n",
    "            else:\n",
    "                skipped_files += 1\n",
    "                print(f\"Skipped file due to inconsistent headers: {filename}\")\n",
    "                continue\n",
    "\n",
    "            files_processed += 1\n",
    "            print(f\"Processed: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            skipped_files += 1\n",
    "\n",
    "    # Save the merged data to the output file\n",
    "    if not merged_data.empty:\n",
    "        merged_data.to_csv(output_file, index=False)\n",
    "        print(f\"\\nMerging complete! Processed {files_processed} files, skipped {skipped_files} files.\")\n",
    "        print(f\"Merged data saved to {output_file}.\")\n",
    "    else:\n",
    "        print(\"\\nNo files were merged. Check the input folder and headers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f00573",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_meta_files_with_batch_debug(input_folder='./data', output_file='./data/full_meta.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_full_meta(file_path, output_file):\n",
    "    # Load the merged data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure 'Batch' column is structured as integers\n",
    "    if 'Batch' in data.columns:\n",
    "        data['Batch'] = pd.to_numeric(data['Batch'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Ensure 'Distance (km)' column is structured as floats\n",
    "    if 'Distance (km)' in data.columns:\n",
    "        data['Distance (km)'] = pd.to_numeric(data['Distance (km)'], errors='coerce')\n",
    "\n",
    "    # Find duplicate names across different batch numbers\n",
    "    if 'Name' in data.columns and 'Batch' in data.columns:\n",
    "        duplicate_names = (\n",
    "            data.groupby('Name')['Batch']\n",
    "            .nunique()\n",
    "            .reset_index()\n",
    "            .query('Batch > 1')\n",
    "            ['Name']\n",
    "        )\n",
    "        \n",
    "        # Filter out entries with the later batch number for these names\n",
    "        duplicates_filtered = data[data['Name'].isin(duplicate_names)]\n",
    "        data = data[~data['Name'].isin(duplicate_names)]\n",
    "        \n",
    "        earliest_batches = duplicates_filtered.groupby('Name')['Batch'].min().reset_index()\n",
    "        duplicates_filtered = duplicates_filtered.merge(earliest_batches, on=['Name', 'Batch'], how='inner')\n",
    "        data = pd.concat([data, duplicates_filtered], ignore_index=True)\n",
    "\n",
    "        print(f\"Filtered out duplicate names across batches: {list(duplicate_names)}\")\n",
    "\n",
    "    # Save the processed data\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f\"Processed data saved to {output_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_full_meta(file_path='./data/full_meta.csv', output_file='./data/processed_full_meta.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_and_sort_athletes(input_file, output_sorted_csv, output_statistics_csv):\n",
    "    # Load data\n",
    "    data = pd.read_csv(input_file)\n",
    "\n",
    "    # Ensure Athlete ID is treated as integers\n",
    "    data['Athlete ID'] = pd.to_numeric(data['Athlete ID'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Ensure Distance (km) column is numeric\n",
    "    data['Distance (km)'] = pd.to_numeric(data['Distance (km)'], errors='coerce').fillna(0)\n",
    "\n",
    "    # Group by Athlete ID to calculate consistency metrics\n",
    "    athlete_groups = data.groupby('Athlete ID')\n",
    "\n",
    "    # Create a DataFrame for athlete statistics\n",
    "    athlete_stats = []\n",
    "    for athlete_id, group in athlete_groups:\n",
    "        total_weeks = len(group)\n",
    "        valid_weeks = (group['Distance (km)'] > 0).sum()\n",
    "        consistency_percentage = (valid_weeks / total_weeks) * 100 if total_weeks > 0 else 0\n",
    "        std_dev = group['Distance (km)'].std() if valid_weeks > 0 else float('inf')\n",
    "        median_distance = group['Distance (km)'].median()\n",
    "        mean_distance = group['Distance (km)'].mean()\n",
    "        max_distance = group['Distance (km)'].max()\n",
    "        median_to_mean_ratio = median_distance / mean_distance if mean_distance > 0 else 0\n",
    "        max_to_median_ratio = max_distance / median_distance if median_distance > 0 else float('inf')\n",
    "        no_data_count = (group['Distance (km)'] == 0).sum()\n",
    "        \n",
    "        athlete_name = group['Name'].iloc[0]  \n",
    "        athlete_stats.append({\n",
    "            'Athlete ID': athlete_id,\n",
    "            'Athlete Name': athlete_name,  \n",
    "            'Consistency Percentage': consistency_percentage,\n",
    "            'Standard Deviation': std_dev,\n",
    "            'Median-to-Mean Ratio': median_to_mean_ratio,\n",
    "            'Max-to-Median Ratio': max_to_median_ratio,\n",
    "            'No Data Count': no_data_count\n",
    "        })\n",
    "\n",
    "    # Create DataFrame of statistics\n",
    "    athlete_stats_df = pd.DataFrame(athlete_stats)\n",
    "\n",
    "    # Sort by Consistency Percentage, then by Standard Deviation, and push \"No Data\" athletes to the bottom\n",
    "    athlete_stats_df.sort_values(\n",
    "        by=['Consistency Percentage', 'Standard Deviation', 'No Data Count'], \n",
    "        ascending=[False, True, True], \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # Create a categorical sorting order based on sorted Athlete ID\n",
    "    sorting_order = pd.Categorical(data['Athlete ID'], categories=athlete_stats_df['Athlete ID'], ordered=True)\n",
    "    sorted_data = data.sort_values(by='Athlete ID', key=lambda x: sorting_order)\n",
    "\n",
    "    # Save the sorted data and statistics\n",
    "    sorted_data.to_csv(output_sorted_csv, index=False)\n",
    "    athlete_stats_df.to_csv(output_statistics_csv, index=False)\n",
    "\n",
    "    print(f\"Sorted original CSV saved to {output_sorted_csv}\")\n",
    "    print(f\"Athlete statistics CSV saved to {output_statistics_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3375bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "process_and_sort_athletes(\n",
    "    input_file='./data/processed_full_meta.csv',\n",
    "    output_sorted_csv='./data/sorted_full_meta.csv',\n",
    "    output_statistics_csv='./data/athlete_statistics.csv'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenium_env",
   "language": "python",
   "name": "selenium_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
